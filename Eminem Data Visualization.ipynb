{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eminem Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, hashing_trick\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Embedding, Flatten, Dense, Dropout, SimpleRNN, GRU, LSTM # if GPU version, check out CUDNN\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data/EDA/Organizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyrics_by_song_with_headers = pd.read_csv('./Eminem Lyrics with Headers', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_by_song_without_headers = pd.read_csv('./Eminem Lyrics without Headers', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lyrics_eminem.txt', 'r')\n",
    "\n",
    "with open('lyrics_eminem.txt', 'r') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "split_data = raw_data.split('\\n')\n",
    "\n",
    "lyrics_by_line = pd.DataFrame(split_data)\n",
    "\n",
    "lyrics_by_line.columns = ['Eminem Lyrics by Line']\n",
    "\n",
    "lyrics_by_line # each sample as a row as in each line is a sample which is why we have so many compared to whole songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyrics_by_song_with_headers # each sample as a row as in each song is a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyrics_by_song_with_headers['y'] = lyrics_by_song_with_headers['lyrics'] + ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_by_song_without_headers # each sample as a row as in each song is a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = lyrics_by_song_with_headers\n",
    "X # lyrics_by_song_with_headers['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_headers = lyrics_by_song_without_headers\n",
    "X_no_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_by_line = lyrics_by_line['Eminem Lyrics by Line']\n",
    "\n",
    "X_by_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lyrics_by_song_with_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGGER NOTE?!: CAN WE SEQUENCE LYRICS BY SONG WITH SEQUENCE TO MATRIX WITH KERAS TOKENIZER CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: We want X to be our vectorized data and for y to be our successful regular text/lyrics, so X and y will be equal before vectorizing, right?!?!?!??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in X.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([str(i) for i in X.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [str(i) for i in X.values] # this gives me 562 strings (each individual string is a song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec of corpus with nltk tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_to_word2vec = X_no_headers['lyrics'].values # for Word2Vec purposes no headers in lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_to_word2vec = [nltk.word_tokenize(str(lyrics).lower()) for lyrics in lyrics_to_word2vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lyrics_to_word2vec = nltk.word_tokenize(raw_data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = Word2Vec(lyrics_to_word2vec, min_count = 5, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(lyrics_to_word2vec, min_count = 5, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('eminem', topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.wv.most_similar('eminem', topn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization T-sne | Code modified from: https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['eminem', 'fuck', 'dick', 'bitch', 'drugs', 'ass', 'violence', 'mom']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in raw_model.wv.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(raw_model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(24, 18))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.8, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=12)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plt.savefig('Similar Words t-sne.png')\n",
    "tsne_plot_similar_words('Similar words from Eminem Corpus', keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ak = []\n",
    "embeddings_ak = []\n",
    "for word in list(raw_model.wv.vocab):\n",
    "    embeddings_ak.append(raw_model.wv[word])\n",
    "    words_ak.append(word)\n",
    "    \n",
    "tsne_ak_2d = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings_ak)\n",
    "\n",
    "def tsne_plot_2d(label, embeddings, words=[], a=1):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, 1))\n",
    "    x = embeddings[:,0]\n",
    "    y = embeddings[:,1]\n",
    "    plt.scatter(x, y, c=colors, alpha=a, label=label)\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, alpha=0.3, xy=(x[i], y[i]), xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"hhh.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot_2d('Eminem Corpus', embeddings_ak_2d, a=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_wp = []\n",
    "embeddings_wp = []\n",
    "for word in list(raw_model.wv.vocab):\n",
    "    embeddings_wp.append(raw_model.wv[word])\n",
    "    words_wp.append(word)\n",
    "    \n",
    "tsne_wp_3d = TSNE(perplexity=30, n_components=3, init='pca', n_iter=3500, random_state=12)\n",
    "embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def tsne_plot_3d(title, label, embeddings, a=1):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, 1))\n",
    "    plt.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], c=colors, alpha=a, label=label)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_3d('Visualizing Embeddings using t-SNE', 'Eminem Lyrics Corpus', embeddings_wp_3d, a=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/Pre-Processing/Encoding Text Data/Lyrics with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1,560 # most words in a single song\n",
    "training_samples = len(songs) # I want to train the model on every song\n",
    "max_words = 21_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words=max_words) # instantiating Tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(songs) # fitting tokenizer to my corpus\n",
    "\n",
    "song_sequences = t.texts_to_sequences(songs) # creating a list of sequences of integers out of my list of strings/lyrics\n",
    "song_matrix = t.sequences_to_matrix(song_sequences)\n",
    "one_hot_lyrics = t.texts_to_matrix(songs, mode='binary') # one hot encoded vectors\n",
    "cvec_lyrics = t.texts_to_matrix(songs, mode='count') # count vectors\n",
    "tfidf_lyrics = t.texts_to_matrix(songs, mode='tfidf') # tfidf vectors\n",
    "# above is my vectorized data of songs. can we use this as our x to train a model and then predict a new y?\n",
    "\n",
    "word_index = t.word_index # saving my token/word index to a variable. word_index is a dict\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens/words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(song_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(song_sequences) # list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(song_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(song_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Word Counts: {t.word_counts}')\n",
    "print(f'Document Count: {t.document_count}')\n",
    "print(f'Word Index: {t.word_index}')\n",
    "print(f'Word Docs: {t.word_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=song_sequences, padding='post')\n",
    "# transforming my list of sequences, 'sequences', into a 2D Numpy array with a maximum length of max_len and making all sequences same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
