{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is a modified version of a tutorial found here:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/?unapproved=525293&moderation-hash=c2d08afe210bc708b6fea629746145cc#comment-525293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL to Google Colab environment for this notebook:\n",
    "\n",
    "https://colab.research.google.com/drive/1HQuIGxAEb9SFsil9DnECkoLiMbc9nHtx#scrollTo=zHtL3Qx3ck5z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSP1Iwr8XgZz"
   },
   "source": [
    "# Eminem RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UqsypHKRnLAJ",
    "outputId": "f55983e7-3bbb-4c28-d5a1-2ade7b31a10f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QscQo_h96Nf"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, hashing_trick\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from random import randint\n",
    "from pickle import load, dump\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "colab_type": "code",
    "id": "7NnOqni0OrtF",
    "outputId": "517d6362-6d35-490f-e525-8941cc66d974"
   },
   "outputs": [],
   "source": [
    "lyrics_by_song_with_headers = pd.read_csv('/content/drive/My Drive/Eminem Lyrics with Headers', index_col=0)\n",
    "lyrics_by_song_with_headers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yFzYSuSJJN57",
    "outputId": "e1921abc-a8ed-4f62-8473-7d2dc5492e91"
   },
   "outputs": [],
   "source": [
    "f = open('/content/drive/My Drive/lyrics_eminem.txt', 'r')\n",
    "\n",
    "with open('/content/drive/My Drive/lyrics_eminem.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "type(text) # 1 huge string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sb2a4woUJK5r"
   },
   "outputs": [],
   "source": [
    "# Create function to clean and tokenize text\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh1BvWFL-dpk"
   },
   "outputs": [],
   "source": [
    "# Create function to save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "r1rfnXfGNY-G",
    "outputId": "482d49c7-3650-4211-b5c0-96a5101c9e0d"
   },
   "outputs": [],
   "source": [
    "# clean my text of lyrics\n",
    "tokens_list = clean_doc(text)\n",
    "print(tokens_list[:200])\n",
    "print('Total Tokens: %d' % len(tokens_list))\n",
    "print('Unique Tokens: %d' % len(set(tokens_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6sOGOJLgJRlo",
    "outputId": "606bb888-eb58-42f5-8c55-9da37d85bd57"
   },
   "outputs": [],
   "source": [
    "# Organizing tokens into sequences\n",
    "seq_length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(seq_length, len(tokens_list)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens_list[i-seq_length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuxlxjaS9_Iq"
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'final_eminem_lyrics.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ClEHHy2SgAV"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filepath):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filepath, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'final_eminem_lyrics.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKRP5PxCS9A-"
   },
   "outputs": [],
   "source": [
    "# Using Keras tokenizer to tokenizer and sequence data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "len(tokenizer.word_index)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIlSqMueTRgt"
   },
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "hPhspEHtTZw3",
    "outputId": "4e72f0f5-0218-46d9-fca5-0c2733e237a7"
   },
   "outputs": [],
   "source": [
    "# Designing LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=seq_length)) # 300 is dimension size\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "0AMNLOxvHa-F",
    "outputId": "e30ff227-d9ed-44ba-9070-066e7b4ffb5a"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udnNPE71B6_h"
   },
   "source": [
    "Finally... FIT THIS MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "F7TYpyKFB547",
    "outputId": "9ac1013f-d5bc-406f-abdc-9cf53a5be07d"
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(X, y, batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "BeAf0ecvIgeG",
    "outputId": "a242a0be-e19a-491c-be66-c089625f1934"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7EuKekiboTN"
   },
   "source": [
    "Re-Loading Data and using Trained Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "zHtL3Qx3ck5z",
    "outputId": "74bb3bfe-dcee-454b-ea03-08aa0799f948"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'final_eminem_lyrics.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDMuxunMihNb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Eminem RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
